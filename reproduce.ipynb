{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5822e700",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# ===================== å¯è°ƒå‚æ•° =====================\n",
    "excel_path   = \"your_dataset.xlsx\"\n",
    "target_col   = \"group\"          # äºŒåˆ†ç±» 0/1\n",
    "cv_folds     = 5\n",
    "random_state = 42\n",
    "rfe_k        = 10\n",
    "rf_top_k     = 10\n",
    "missing_strategy = \"median\"     # median / mean\n",
    "\n",
    "# å¯¼å‡ºæ–‡ä»¶å\n",
    "export_xlsx  = \"FeatureSelection_Report.xlsx\"\n",
    "# ===================================================\n",
    "\n",
    "# ï¼ˆå¯é€‰ï¼‰å±è”½ sklearn FutureWarningï¼ˆä¸å½±å“ç»“æœï¼‰\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"sklearn\")\n",
    "\n",
    "# --------------------- å·¥å…·å‡½æ•° ---------------------\n",
    "def coerce_binary_series(s: pd.Series) -> pd.Series:\n",
    "    if pd.api.types.is_bool_dtype(s):\n",
    "        return s.astype(int)\n",
    "    if pd.api.types.is_numeric_dtype(s):\n",
    "        return s\n",
    "\n",
    "    s_str = s.astype(str).str.strip().str.lower()\n",
    "    mapping = {\n",
    "        \"0\": 0, \"1\": 1,\n",
    "        \"no\": 0, \"yes\": 1,\n",
    "        \"n\": 0, \"y\": 1,\n",
    "        \"false\": 0, \"true\": 1,\n",
    "        \"negative\": 0, \"positive\": 1,\n",
    "        \"neg\": 0, \"pos\": 1,\n",
    "        \"é˜´æ€§\": 0, \"é˜³æ€§\": 1,\n",
    "        \"å¦\": 0, \"æ˜¯\": 1,\n",
    "        \"æ— \": 0, \"æœ‰\": 1,\n",
    "    }\n",
    "    uniq = s_str.dropna().unique()\n",
    "    hit_ratio = np.mean([u in mapping for u in uniq]) if len(uniq) > 0 else 0.0\n",
    "    if hit_ratio >= 0.8:\n",
    "        return s_str.map(mapping)\n",
    "    return s\n",
    "\n",
    "def check_binary_target(y: pd.Series) -> np.ndarray:\n",
    "    if pd.api.types.is_bool_dtype(y):\n",
    "        y = y.astype(int)\n",
    "\n",
    "    if not pd.api.types.is_numeric_dtype(y):\n",
    "        y2 = coerce_binary_series(y)\n",
    "        if pd.api.types.is_numeric_dtype(y2):\n",
    "            y = y2\n",
    "        else:\n",
    "            raise ValueError(\"group ä¸æ˜¯æ•°å€¼ 0/1ï¼Œä¹Ÿæ— æ³•è‡ªåŠ¨è¯†åˆ«ä¸ºäºŒåˆ†ç±»æ–‡æœ¬ã€‚è¯·å…ˆæŠŠ group å¤„ç†æˆ 0/1ã€‚\")\n",
    "\n",
    "    y = pd.to_numeric(y, errors=\"coerce\")\n",
    "    if y.isna().any():\n",
    "        raise ValueError(\"group å«æœ‰æ— æ³•è½¬æˆæ•°å€¼çš„å€¼ï¼ˆNaNï¼‰ã€‚è¯·æ£€æŸ¥ group åˆ—ã€‚\")\n",
    "\n",
    "    uniq = sorted(y.unique().tolist())\n",
    "    if set(uniq) != {0, 1}:\n",
    "        raise ValueError(f\"group ä¸æ˜¯æ ‡å‡†äºŒåˆ†ç±» 0/1ï¼Œå½“å‰å”¯ä¸€å€¼ä¸ºï¼š{uniq}\")\n",
    "\n",
    "    return y.astype(int).values\n",
    "\n",
    "def preprocess_X(df: pd.DataFrame, target_col: str) -> pd.DataFrame:\n",
    "    X = df.drop(columns=[target_col]).copy()\n",
    "\n",
    "    for col in X.columns:\n",
    "        X[col] = coerce_binary_series(X[col])\n",
    "\n",
    "    X = X.apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "    if missing_strategy == \"mean\":\n",
    "        fill_vals = X.mean(numeric_only=True)\n",
    "    else:\n",
    "        fill_vals = X.median(numeric_only=True)\n",
    "\n",
    "    X = X.fillna(fill_vals)\n",
    "    X = X.fillna(0)\n",
    "    return X\n",
    "\n",
    "# --------------------- è¯»å–æ•°æ® ---------------------\n",
    "df_raw = pd.read_excel(excel_path)\n",
    "y = check_binary_target(df_raw[target_col])\n",
    "X = preprocess_X(df_raw, target_col)\n",
    "\n",
    "# æ•°æ®æ¦‚å†µ\n",
    "n = len(df_raw)\n",
    "pos_rate = float(np.mean(y))\n",
    "missing_rate = df_raw.drop(columns=[target_col]).isna().mean().sort_values(ascending=False)\n",
    "\n",
    "data_overview = pd.DataFrame({\n",
    "    \"Metric\": [\"N\", \"Positive rate (y=1)\", \"Negative rate (y=0)\", \"Num features\"],\n",
    "    \"Value\": [n, pos_rate, 1 - pos_rate, X.shape[1]]\n",
    "})\n",
    "\n",
    "feature_list = pd.DataFrame({\"Feature\": X.columns})\n",
    "\n",
    "missing_table = missing_rate.reset_index()\n",
    "missing_table.columns = [\"Feature (raw)\", \"Missing rate (raw)\"]\n",
    "\n",
    "# æ ‡å‡†åŒ–\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# ===================================================\n",
    "# 1) L1-LogisticCV (AUC)  â€”â€” çº¯ L1 ç”¨ elasticnet + l1_ratios=(1.0,)\n",
    "# ===================================================\n",
    "l1_logit_cv = LogisticRegressionCV(\n",
    "    Cs=10,\n",
    "    penalty=\"elasticnet\",\n",
    "    solver=\"saga\",\n",
    "    l1_ratios=(1.0,),\n",
    "    cv=cv_folds,\n",
    "    scoring=\"roc_auc\",\n",
    "    class_weight=\"balanced\",\n",
    "    n_jobs=-1,\n",
    "    max_iter=20000,\n",
    "    random_state=random_state,\n",
    "    use_legacy_attributes=True,\n",
    ")\n",
    "l1_logit_cv.fit(X_scaled, y)\n",
    "\n",
    "coef = l1_logit_cv.coef_\n",
    "coef_vec = coef.ravel() if coef.ndim == 1 else np.max(coef, axis=0)\n",
    "coef_abs = np.abs(coef).ravel() if coef.ndim == 1 else np.max(np.abs(coef), axis=0)\n",
    "\n",
    "l1_selected = X.columns[coef_abs > 1e-8]\n",
    "\n",
    "l1_coef_table = pd.DataFrame({\n",
    "    \"Feature\": X.columns,\n",
    "    \"Coef (aggregated)\": coef_vec,\n",
    "    \"AbsCoef\": coef_abs\n",
    "}).sort_values(\"AbsCoef\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "l1_selected_table = l1_coef_table[l1_coef_table[\"AbsCoef\"] > 1e-8].reset_index(drop=True)\n",
    "\n",
    "# ===================================================\n",
    "# 2) Random Forest importance\n",
    "# ===================================================\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=500,\n",
    "    random_state=random_state,\n",
    "    n_jobs=-1,\n",
    "    class_weight=\"balanced_subsample\"\n",
    ")\n",
    "rf.fit(X, y)\n",
    "\n",
    "rf_importance = pd.Series(rf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "rf_imp_table = rf_importance.reset_index()\n",
    "rf_imp_table.columns = [\"Feature\", \"RF importance\"]\n",
    "\n",
    "rf_top_table = rf_imp_table.head(rf_top_k).copy()\n",
    "rf_selected = rf_top_table[\"Feature\"].tolist()\n",
    "\n",
    "# ===================================================\n",
    "# 3) SVM-RFE\n",
    "# ===================================================\n",
    "svm = SVC(kernel=\"linear\")\n",
    "rfe_svm = RFE(estimator=svm, n_features_to_select=rfe_k)\n",
    "rfe_svm.fit(X_scaled, y)\n",
    "\n",
    "svm_selected = X.columns[rfe_svm.support_].tolist()\n",
    "svm_selected_table = pd.DataFrame({\"Feature\": svm_selected})\n",
    "\n",
    "# ===================================================\n",
    "# 4) Logistic-RFE\n",
    "# ===================================================\n",
    "log_reg = LogisticRegression(\n",
    "    solver=\"liblinear\",\n",
    "    class_weight=\"balanced\",\n",
    "    random_state=random_state,\n",
    "    max_iter=10000\n",
    ")\n",
    "rfe_logit = RFE(estimator=log_reg, n_features_to_select=rfe_k)\n",
    "rfe_logit.fit(X_scaled, y)\n",
    "\n",
    "logit_rfe_selected = X.columns[rfe_logit.support_].tolist()\n",
    "logit_rfe_selected_table = pd.DataFrame({\"Feature\": logit_rfe_selected})\n",
    "\n",
    "# ===================================================\n",
    "# 5) ç»¼åˆï¼šäº¤é›† & ä¸€è‡´æ€§ï¼ˆâ‰¥3ï¼‰\n",
    "# ===================================================\n",
    "method_sets = {\n",
    "    \"L1-Logistic(CV)\": set(l1_selected),\n",
    "    \"RF(TopK)\": set(rf_selected),\n",
    "    \"SVM-RFE\": set(svm_selected),\n",
    "    \"Logistic-RFE\": set(logit_rfe_selected),\n",
    "}\n",
    "\n",
    "intersection_all4 = sorted(set.intersection(*method_sets.values()))\n",
    "intersection_table = pd.DataFrame({\"Feature\": intersection_all4})\n",
    "\n",
    "all_feats = sorted(set().union(*method_sets.values()))\n",
    "appear_counts = {f: sum(f in s for s in method_sets.values()) for f in all_feats}\n",
    "\n",
    "consensus_ge3 = sorted([f for f, c in appear_counts.items() if c >= 3])\n",
    "consensus_table = pd.DataFrame({\"Feature\": consensus_ge3})\n",
    "\n",
    "appear_table = (\n",
    "    pd.DataFrame({\"Feature\": all_feats, \"Appearances\": [appear_counts[f] for f in all_feats]})\n",
    "      .sort_values([\"Appearances\", \"Feature\"], ascending=[False, True])\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# æ–¹æ³•-ç‰¹å¾çŸ©é˜µï¼ˆæ–¹ä¾¿è®ºæ–‡é™„å½•/å®¡ç¨¿é—®ä¸€è‡´æ€§ï¼‰\n",
    "matrix = []\n",
    "for f in all_feats:\n",
    "    row = {\"Feature\": f}\n",
    "    for m, s in method_sets.items():\n",
    "        row[m] = 1 if f in s else 0\n",
    "    row[\"Appearances\"] = appear_counts[f]\n",
    "    matrix.append(row)\n",
    "method_matrix = pd.DataFrame(matrix).sort_values([\"Appearances\", \"Feature\"], ascending=[False, True]).reset_index(drop=True)\n",
    "\n",
    "# å‚æ•°è®°å½•\n",
    "params_table = pd.DataFrame({\n",
    "    \"Parameter\": [\"excel_path\", \"target_col\", \"cv_folds\", \"random_state\", \"rfe_k\", \"rf_top_k\", \"missing_strategy\", \"scoring(L1-CV)\"],\n",
    "    \"Value\": [excel_path, target_col, cv_folds, random_state, rfe_k, rf_top_k, missing_strategy, \"roc_auc\"]\n",
    "})\n",
    "\n",
    "# ===================================================\n",
    "# 6) å¯¼å‡ºåˆ° Excelï¼ˆå¤š Sheetï¼‰\n",
    "# ===================================================\n",
    "with pd.ExcelWriter(export_xlsx, engine=\"openpyxl\") as writer:\n",
    "    params_table.to_excel(writer, sheet_name=\"00_Params\", index=False)\n",
    "    data_overview.to_excel(writer, sheet_name=\"01_DataOverview\", index=False)\n",
    "    feature_list.to_excel(writer, sheet_name=\"02_FeatureList\", index=False)\n",
    "    missing_table.to_excel(writer, sheet_name=\"03_MissingRaw\", index=False)\n",
    "\n",
    "    l1_selected_table.to_excel(writer, sheet_name=\"10_L1_Selected\", index=False)\n",
    "    l1_coef_table.to_excel(writer, sheet_name=\"11_L1_AllCoefs\", index=False)\n",
    "\n",
    "    rf_top_table.to_excel(writer, sheet_name=\"20_RF_TopK\", index=False)\n",
    "    rf_imp_table.to_excel(writer, sheet_name=\"21_RF_AllImportance\", index=False)\n",
    "\n",
    "    svm_selected_table.to_excel(writer, sheet_name=\"30_SVM_RFE\", index=False)\n",
    "    logit_rfe_selected_table.to_excel(writer, sheet_name=\"31_Logit_RFE\", index=False)\n",
    "\n",
    "    intersection_table.to_excel(writer, sheet_name=\"40_Intersection_All4\", index=False)\n",
    "    consensus_table.to_excel(writer, sheet_name=\"41_Consensus_GE3\", index=False)\n",
    "    appear_table.to_excel(writer, sheet_name=\"42_AppearCounts\", index=False)\n",
    "    method_matrix.to_excel(writer, sheet_name=\"43_MethodMatrix\", index=False)\n",
    "\n",
    "print(f\"âœ… å·²å¯¼å‡º Excel æŠ¥å‘Šï¼š{export_xlsx}\")\n",
    "print(\"ä¸¥æ ¼äº¤é›†ï¼ˆAll4ï¼‰:\", intersection_all4)\n",
    "print(\"ä¸€è‡´æ€§â‰¥3:\", consensus_ge3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aece3fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re, json, joblib\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# ====================== ä½ çš„ä¸¤ä¸ªç‰¹å¾é›†ï¼ˆæ¥è‡ª FeatureSelection_Report.xlsxï¼‰ ======================\n",
    "primary_features = [\n",
    "    'your_data'\n",
    "]\n",
    "\n",
    "extended_features = [\n",
    "    'your_data'\n",
    "]\n",
    "\n",
    "train_path = \"your_dataset.xlsx\"\n",
    "test_path  = \"your_dataset.xlsx\"\n",
    "target_col = \"group\"\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# ====================== å·¥å…·å‡½æ•° ======================\n",
    "def ensure_binary_01(y: pd.Series, name=\"target\"):\n",
    "    y_num = pd.to_numeric(y, errors=\"coerce\")\n",
    "    if y_num.isna().any():\n",
    "        bad = y[y_num.isna()].unique()\n",
    "        raise ValueError(f\"{name} å«æ— æ³•è½¬ä¸ºæ•°å€¼çš„å€¼ï¼š{bad}\")\n",
    "    uniq = sorted(y_num.unique().tolist())\n",
    "    if set(uniq) != {0, 1}:\n",
    "        raise ValueError(f\"{name} ä¸æ˜¯æ ‡å‡†äºŒåˆ†ç±»0/1ï¼Œå½“å‰å”¯ä¸€å€¼={uniq}\")\n",
    "    return y_num.astype(int).values\n",
    "\n",
    "def to_numeric_df(X: pd.DataFrame) -> pd.DataFrame:\n",
    "    X = X.copy()\n",
    "    for c in X.columns:\n",
    "        X[c] = pd.to_numeric(X[c], errors=\"coerce\")\n",
    "    return X\n",
    "\n",
    "def assert_no_missing(X: pd.DataFrame, name=\"X\"):\n",
    "    if X.isna().any().any():\n",
    "        miss_cols = X.columns[X.isna().any()].tolist()\n",
    "        miss_rate = X[miss_cols].isna().mean().sort_values(ascending=False)\n",
    "        raise ValueError(f\"{name} ä»å­˜åœ¨ç¼ºå¤±å€¼ï¼š{miss_rate.to_dict()}\")\n",
    "\n",
    "def calculate_metrics(y_true, y_pred, y_score):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    sens = tp/(tp+fn) if (tp+fn)>0 else 0.0\n",
    "    spec = tn/(tn+fp) if (tn+fp)>0 else 0.0\n",
    "    acc  = (tp+tn)/(tp+tn+fp+fn)\n",
    "    ppv  = tp/(tp+fp) if (tp+fp)>0 else 0.0\n",
    "    npv  = tn/(tn+fn) if (tn+fn)>0 else 0.0\n",
    "    f1   = 2*tp/(2*tp+fp+fn) if (2*tp+fp+fn)>0 else 0.0\n",
    "    auc  = roc_auc_score(y_true, y_score) if len(np.unique(y_true))>1 else 0.0\n",
    "    return {'AUC':auc,'Sensitivity':sens,'Specificity':spec,'Accuracy':acc,'PPV':ppv,'NPV':npv,'F1':f1}\n",
    "\n",
    "def best_threshold(y_true, y_score, mode='f1'):\n",
    "    thresholds = np.linspace(0.01, 0.99, 99)\n",
    "    best_thr, best_score = 0.5, -np.inf\n",
    "    for t in thresholds:\n",
    "        y_pred = (y_score >= t).astype(int)\n",
    "        m = calculate_metrics(y_true, y_pred, y_score)\n",
    "        score = m['F1'] if mode == 'f1' else (m['Sensitivity'] + m['Specificity'] - 1)\n",
    "        if score > best_score:\n",
    "            best_score, best_thr = score, t\n",
    "    return float(best_thr)\n",
    "\n",
    "def get_scores(model, X_df):\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        return model.predict_proba(X_df)[:, 1]\n",
    "    if hasattr(model, \"decision_function\"):\n",
    "        return model.decision_function(X_df)\n",
    "    raise ValueError(f\"{type(model).__name__} ç¼ºå°‘ predict_proba/decision_function\")\n",
    "\n",
    "def compute_spw(y):\n",
    "    pos = np.sum(y == 1)\n",
    "    neg = np.sum(y == 0)\n",
    "    return max((neg / pos), 1.0) if pos > 0 else 1.0\n",
    "\n",
    "def oof_scores_fixed_params(base_estimator, best_params, X_df, y_arr, cv):\n",
    "    \"\"\"\n",
    "    ç”¨ best_params å›ºå®šè¶…å‚ï¼Œåœ¨å¼€å‘é›†å…¨é‡ä¸Šåš out-of-fold é¢„æµ‹ï¼Œè¿”å›æ¯ä¸ªæ ·æœ¬çš„ OOF score\n",
    "    \"\"\"\n",
    "    oof = np.full(len(y_arr), np.nan, dtype=float)\n",
    "    for tr_idx, va_idx in cv.split(X_df, y_arr):\n",
    "        m = clone(base_estimator)\n",
    "        m.set_params(**best_params)\n",
    "\n",
    "        # XGBoostï¼šscale_pos_weight å»ºè®®æŒ‰foldè®­ç»ƒé›†è®¡ç®—\n",
    "        if isinstance(m, XGBClassifier):\n",
    "            m.set_params(scale_pos_weight=compute_spw(y_arr[tr_idx]))\n",
    "\n",
    "        m.fit(X_df.iloc[tr_idx], y_arr[tr_idx])\n",
    "        oof[va_idx] = get_scores(m, X_df.iloc[va_idx])\n",
    "\n",
    "    if np.isnan(oof).any():\n",
    "        raise RuntimeError(\"OOF scores å­˜åœ¨ NaNï¼Œè¯·æ£€æŸ¥æ•°æ®/ç´¢å¼•ã€‚\")\n",
    "    return oof\n",
    "\n",
    "# ====================== CV & BayesSearch è®¾ç½® ======================\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "bayes_common = dict(\n",
    "    n_iter=30,\n",
    "    cv=cv,\n",
    "    scoring='roc_auc',\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1,\n",
    "    optimizer_kwargs={'acq_optimizer': 'sampling'}\n",
    ")\n",
    "\n",
    "# ====================== æ¨¡å‹ï¼ˆè¿™é‡Œä¸æ”¾ StandardScalerï¼Œå› ä¸ºä½ å·²ç»æ•°å€¼åŒ–ä¸”é‡çº²å·®å¼‚ä¸å½±å“æ ‘æ¨¡å‹ï¼›ï¼‰\n",
    "# å¯¹ LR/SVM/KNN æˆ‘ä»¬ç”¨ Pipeline+Scalerï¼Œæ›´è§„èŒƒã€‚æ ‘æ¨¡å‹ passthroughã€‚\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def make_scaler():\n",
    "    return StandardScaler()\n",
    "\n",
    "models_config = {\n",
    "    'Logistic Regression': {\n",
    "        'estimator': Pipeline([\n",
    "            ('scaler', make_scaler()),\n",
    "            ('clf', LogisticRegression(\n",
    "                random_state=RANDOM_STATE, class_weight='balanced', max_iter=5000\n",
    "            ))\n",
    "        ]),\n",
    "        'search_spaces': {'clf__C': Real(1e-2, 1e2, prior='log-uniform')}\n",
    "    },\n",
    "\n",
    "    'Random Forest': {\n",
    "        'estimator': Pipeline([\n",
    "            ('scaler', 'passthrough'),\n",
    "            ('clf', RandomForestClassifier(\n",
    "                random_state=RANDOM_STATE, class_weight='balanced'\n",
    "            ))\n",
    "        ]),\n",
    "        'search_spaces': {\n",
    "            'clf__n_estimators': Integer(200, 800),\n",
    "            'clf__max_depth': Integer(3, 30),\n",
    "            'clf__min_samples_split': Integer(2, 30),\n",
    "            'clf__min_samples_leaf': Integer(1, 15),\n",
    "            'clf__max_features': Categorical(['sqrt', 'log2', None])\n",
    "        }\n",
    "    },\n",
    "\n",
    "    'XGBoost': {\n",
    "        'estimator': Pipeline([\n",
    "            ('scaler', 'passthrough'),\n",
    "            ('clf', XGBClassifier(\n",
    "                random_state=RANDOM_STATE,\n",
    "                eval_metric='logloss',\n",
    "                tree_method='hist'\n",
    "            ))\n",
    "        ]),\n",
    "        'search_spaces': {\n",
    "            'clf__n_estimators': Integer(200, 1000),\n",
    "            'clf__max_depth': Integer(2, 10),\n",
    "            'clf__learning_rate': Real(0.01, 0.3, prior='log-uniform'),\n",
    "            'clf__subsample': Real(0.6, 1.0),\n",
    "            'clf__colsample_bytree': Real(0.5, 1.0),\n",
    "            'clf__reg_lambda': Real(1e-3, 10, prior='log-uniform')\n",
    "        }\n",
    "    },\n",
    "\n",
    "    'SVM': {\n",
    "        'estimator': Pipeline([\n",
    "            ('scaler', make_scaler()),\n",
    "            ('clf', SVC(\n",
    "                random_state=RANDOM_STATE,\n",
    "                probability=True,\n",
    "                class_weight='balanced'\n",
    "            ))\n",
    "        ]),\n",
    "        'search_spaces': [\n",
    "            {'clf__kernel': Categorical(['linear']),\n",
    "             'clf__C': Real(1e-2, 1e2, prior='log-uniform')},\n",
    "            {'clf__kernel': Categorical(['rbf']),\n",
    "             'clf__C': Real(1e-2, 1e2, prior='log-uniform'),\n",
    "             'clf__gamma': Real(1e-3, 1, prior='log-uniform')}\n",
    "        ]\n",
    "    },\n",
    "\n",
    "    'KNN': {\n",
    "        'estimator': Pipeline([\n",
    "            ('scaler', make_scaler()),\n",
    "            ('clf', KNeighborsClassifier())\n",
    "        ]),\n",
    "        'search_spaces': {\n",
    "            'clf__n_neighbors': Integer(3, 40),\n",
    "            'clf__weights': Categorical(['uniform', 'distance']),\n",
    "            'clf__metric': Categorical(['euclidean', 'manhattan'])\n",
    "        }\n",
    "    },\n",
    "\n",
    "    'Decision Tree': {\n",
    "        'estimator': Pipeline([\n",
    "            ('scaler', 'passthrough'),\n",
    "            ('clf', DecisionTreeClassifier(\n",
    "                random_state=RANDOM_STATE, class_weight='balanced'\n",
    "            ))\n",
    "        ]),\n",
    "        'search_spaces': {\n",
    "            'clf__max_depth': Integer(3, 30),\n",
    "            'clf__min_samples_split': Integer(2, 30),\n",
    "            'clf__min_samples_leaf': Integer(1, 15),\n",
    "            'clf__criterion': Categorical(['gini', 'entropy', 'log_loss'])\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# ====================== è¯»å–æ•°æ® ======================\n",
    "df_train = pd.read_excel(train_path)\n",
    "df_test  = pd.read_excel(test_path)\n",
    "\n",
    "y_dev = ensure_binary_01(df_train[target_col], name=target_col)\n",
    "y_val = ensure_binary_01(df_test[target_col], name=f\"{target_col}\")\n",
    "\n",
    "# ====================== ä¸»å‡½æ•°ï¼šå¼€å‘é›†å…¨é‡ OOF é€‰é˜ˆå€¼ + å¤–æµ‹ ======================\n",
    "def run_set(feature_list, tag):\n",
    "    print(f\"\\n==================== {tag}ï¼ˆp={len(feature_list)}ï¼‰====================\")\n",
    "\n",
    "    # å–ç‰¹å¾ï¼ˆå¼€å‘é›†å…¨é‡ + valï¼‰\n",
    "    X_dev = df_train[feature_list].copy()\n",
    "    X_val = df_test[feature_list].copy()\n",
    "\n",
    "    X_dev = to_numeric_df(X_dev)\n",
    "    X_val = to_numeric_df(X_val)\n",
    "\n",
    "    assert_no_missing(X_dev, name=f\"X_dev({tag})\")\n",
    "    assert_no_missing(X_val, name=f\"X_val({tag})\")\n",
    "\n",
    "    results_oof = []\n",
    "    results_val = []\n",
    "    best_models = {}\n",
    "    best_params_all = {}\n",
    "    thresholds_f1 = {}\n",
    "\n",
    "    print(\"å¼€å§‹ BayesSearchï¼ˆä»…å¼€å‘é›†ï¼‰+ OOF é˜ˆå€¼ï¼ˆå¼€å‘é›†å†…ï¼‰+  å¤–éƒ¨éªŒè¯â€¦\")\n",
    "\n",
    "    for name, cfg in models_config.items():\n",
    "        print(f\"\\nğŸ”„ {name} ...\")\n",
    "\n",
    "        est = cfg['estimator']\n",
    "\n",
    "        # XGBoostï¼šscale_pos_weight å…ˆæŒ‰å…¨å¼€å‘é›†è®¾ç½®ï¼ˆBayesSearchä¸­ä¼šåœ¨æ¯æ¬¡fitçš„è®­ç»ƒfoldé‡Œå†è¢«cloneè¦†ç›–ä¸äº†ï¼‰\n",
    "        # è¿™é‡Œä¸å¼ºåˆ¶ï¼Œåé¢ OOF å’Œ final fit ä¼šæŒ‰fold/å…¨é‡å†è®¾ç½®ä¸€æ¬¡ã€‚\n",
    "        if name == 'XGBoost':\n",
    "            est = clone(est)\n",
    "            est.set_params(clf__scale_pos_weight=compute_spw(y_dev))\n",
    "\n",
    "        bayes = BayesSearchCV(\n",
    "            estimator=est,\n",
    "            search_spaces=cfg['search_spaces'],\n",
    "            **bayes_common\n",
    "        )\n",
    "        bayes.fit(X_dev, y_dev)\n",
    "\n",
    "        best_params = bayes.best_params_\n",
    "        best_params_all[name] = best_params\n",
    "\n",
    "        # --------- OOF scoresï¼ˆç”¨ best_params å›ºå®šï¼‰---------\n",
    "        oof = oof_scores_fixed_params(\n",
    "            base_estimator=cfg['estimator'],\n",
    "            best_params=best_params,\n",
    "            X_df=X_dev,\n",
    "            y_arr=y_dev,\n",
    "            cv=cv\n",
    "        )\n",
    "        thr = best_threshold(y_dev, oof, mode='f1')\n",
    "        thresholds_f1[name] = thr\n",
    "\n",
    "        oof_pred = (oof >= thr).astype(int)\n",
    "        m_oof = calculate_metrics(y_dev, oof_pred, oof)\n",
    "        results_oof.append({'Model': name, 'Thr@F1(OOF)': thr, **m_oof})\n",
    "\n",
    "        # --------- Final modelï¼šç”¨å…¨å¼€å‘é›†æ‹Ÿåˆï¼ˆå›ºå®š best_paramsï¼‰---------\n",
    "        final_model = clone(cfg['estimator'])\n",
    "        final_model.set_params(**best_params)\n",
    "\n",
    "        if name == 'XGBoost':\n",
    "            final_model.set_params(clf__scale_pos_weight=compute_spw(y_dev))\n",
    "\n",
    "        final_model.fit(X_dev, y_dev)\n",
    "        best_models[name] = final_model\n",
    "\n",
    "        # --------- external validationï¼ˆå›ºå®šé˜ˆå€¼ï¼‰---------\n",
    "        te_score = get_scores(final_model, X_val)\n",
    "        te_pred = (te_score >= thr).astype(int)\n",
    "        m_te = calculate_metrics(y_val, te_pred, te_score)\n",
    "        results_val.append({'Model': name, 'Thr@F1(OOF)': thr, **m_te})\n",
    "\n",
    "        print(f\"âœ… OOF AUC={m_oof['AUC']:.4f}, Thr={thr:.2f} | val AUC={m_te['AUC']:.4f}, F1={m_te['F1']:.4f}\")\n",
    "\n",
    "    # æ±‡æ€» + ä¿å­˜\n",
    "    oof_df = pd.DataFrame(results_oof).sort_values('AUC', ascending=False).reset_index(drop=True)\n",
    "    te_df  = pd.DataFrame(results_val).sort_values('AUC', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    out_xlsx = f\"model_performance_{tag}_OOF_TRIPOD.xlsx\"\n",
    "    with pd.ExcelWriter(out_xlsx) as writer:\n",
    "        oof_df.to_excel(writer, sheet_name='DEV_OOF_f1', index=False)\n",
    "        te_df.to_excel(writer, sheet_name='TEST2024_fixedThr', index=False)\n",
    "        pd.DataFrame(best_params_all).T.to_excel(writer, sheet_name='best_params')\n",
    "        pd.DataFrame({'Model': list(thresholds_f1.keys()),\n",
    "                      'Thr@F1(OOF)': list(thresholds_f1.values())}).to_excel(writer, sheet_name='thresholds_f1', index=False)\n",
    "\n",
    "    for model_name, model in best_models.items():\n",
    "        slug = re.sub(r'[^A-Za-z0-9_-]+', '_', model_name.strip()).lower()\n",
    "        joblib.dump(model, f\"best_model_{tag}_{slug}.pkl\")\n",
    "\n",
    "    with open(f'best_model_thresholds_f1_{tag}_OOF_TRIPOD.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump({k: float(v) for k, v in thresholds_f1.items()}, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"\\nâœ… å®Œæˆï¼š{out_xlsx}ã€best_model_{tag}_*.pklã€best_model_thresholds_f1_{tag}_OOF_TRIPOD.json\")\n",
    "    return oof_df, te_df\n",
    "\n",
    "\n",
    "# ====================== è·‘ä¸¤å¥—ç‰¹å¾é›† ======================\n",
    "run_set(primary_features, tag=\"primary_5\")\n",
    "run_set(extended_features, tag=\"extended_8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9521d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# === é…ç½® ===\n",
    "MODEL_PKL = \"best_model_primary_5_random_forest.pkl\"\n",
    "DEV_XLSX = \"your_dataset.xlsx\"\n",
    "TEST_XLSX = \"your_dataset.xlsx\"\n",
    "TARGET = \"group\"\n",
    "\n",
    "FEATURES = [\n",
    "    'your_data'\n",
    "]\n",
    "\n",
    "# === è¯»å–æ•°æ® ===\n",
    "df_dev = pd.read_excel(DEV_XLSX)\n",
    "df_test = pd.read_excel(TEST_XLSX)\n",
    "\n",
    "X_dev = df_dev[FEATURES].apply(pd.to_numeric)\n",
    "y_dev = df_dev[TARGET].values\n",
    "\n",
    "X_test = df_test[FEATURES].apply(pd.to_numeric)\n",
    "y_test = df_test[TARGET].values\n",
    "\n",
    "model = joblib.load(MODEL_PKL)\n",
    "\n",
    "# === OOF ROC (Development) ===\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "oof = np.zeros(len(y_dev))\n",
    "\n",
    "for train_idx, val_idx in cv.split(X_dev, y_dev):\n",
    "    m = model\n",
    "    m.fit(X_dev.iloc[train_idx], y_dev[train_idx])\n",
    "    oof[val_idx] = m.predict_proba(X_dev.iloc[val_idx])[:,1]\n",
    "\n",
    "fpr_dev, tpr_dev, _ = roc_curve(y_dev, oof)\n",
    "auc_dev = roc_auc_score(y_dev, oof)\n",
    "\n",
    "# ===  ROC ===\n",
    "y_test_prob = model.predict_proba(X_test)[:,1]\n",
    "fpr_test, tpr_test, _ = roc_curve(y_test, y_test_prob)\n",
    "auc_test = roc_auc_score(y_test, y_test_prob)\n",
    "\n",
    "# === ç”»å›¾ ===\n",
    "plt.figure()\n",
    "plt.plot(fpr_dev, tpr_dev, label=f'Development (AUC={auc_dev:.3f})')\n",
    "plt.plot(fpr_test, tpr_test, label=f'Validation (AUC={auc_test:.3f})')\n",
    "plt.plot([0,1],[0,1],'k--')\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"Figure_ROC.tiff\", dpi=600)\n",
    "plt.savefig(\"Figure_ROC.png\", dpi=600)\n",
    "plt.close()\n",
    "\n",
    "print(\"ROC figure saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52552f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.base import clone\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import brier_score_loss\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# ===================== é…ç½® =====================\n",
    "MODEL_PKL = \"best_model_primary_5_random_forest.pkl\"   # æ”¹æˆä½ çš„å®é™…æ–‡ä»¶å\n",
    "DEV_XLSX  = \"your_dataset.xlsx\"\n",
    "TEST_XLSX = \"your_dataset.xlsx\"\n",
    "TARGET_COL = \"group\"\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "FEATURES = [\n",
    "    'your_data'\n",
    "]\n",
    "\n",
    "# ===================== å·¥å…·å‡½æ•° =====================\n",
    "def to_numeric_no_missing(df, cols, name=\"X\"):\n",
    "    X = df[cols].copy()\n",
    "    for c in cols:\n",
    "        X[c] = pd.to_numeric(X[c], errors=\"coerce\")\n",
    "    if X.isna().any().any():\n",
    "        miss = X.isna().mean().to_dict()\n",
    "        raise ValueError(f\"{name} still has missing values: {miss}\")\n",
    "    return X\n",
    "\n",
    "def oof_predict_proba(base_model, X, y, cv):\n",
    "    oof = np.full(len(y), np.nan, dtype=float)\n",
    "    for tr, va in cv.split(X, y):\n",
    "        m = clone(base_model)\n",
    "        m.fit(X.iloc[tr], y[tr])\n",
    "        oof[va] = m.predict_proba(X.iloc[va])[:, 1]\n",
    "    if np.isnan(oof).any():\n",
    "        raise RuntimeError(\"OOF probabilities contain NaN.\")\n",
    "    return oof\n",
    "\n",
    "def calibration_slope_intercept(y, p):\n",
    "    \"\"\"\n",
    "    Fit: logit(P(Y=1)) = intercept + slope * logit(p)\n",
    "    è¿”å› (intercept, slope)\n",
    "    \"\"\"\n",
    "    eps = 1e-6\n",
    "    p = np.clip(p, eps, 1 - eps)\n",
    "    x = np.log(p / (1 - p)).reshape(-1, 1)\n",
    "\n",
    "    # sklearn çš„æ— æƒ©ç½šLRï¼šä¸åŒç‰ˆæœ¬ sklearn å‚æ•°ç•¥æœ‰å·®å¼‚ï¼Œä¸‹é¢åšå…¼å®¹\n",
    "    try:\n",
    "        lr = LogisticRegression(penalty=None, solver=\"lbfgs\", max_iter=2000)\n",
    "    except TypeError:\n",
    "        lr = LogisticRegression(penalty=\"none\", solver=\"lbfgs\", max_iter=2000)\n",
    "\n",
    "    lr.fit(x, y)\n",
    "    intercept = float(lr.intercept_[0])\n",
    "    slope = float(lr.coef_[0][0])\n",
    "    return intercept, slope\n",
    "\n",
    "def plot_calibration(y_dev, p_dev, y_test, p_test, n_bins=10, out_prefix=\"Figure_Calibration\"):\n",
    "    # åˆ†ç®±æ ¡å‡†æ›²çº¿\n",
    "    frac_pos_dev, mean_pred_dev = calibration_curve(y_dev, p_dev, n_bins=n_bins, strategy=\"quantile\")\n",
    "    frac_pos_te,  mean_pred_te  = calibration_curve(y_test, p_test, n_bins=n_bins, strategy=\"quantile\")\n",
    "\n",
    "    # ç”»å›¾\n",
    "    plt.figure()\n",
    "    plt.plot([0, 1], [0, 1], linestyle=\"--\", linewidth=1)  # ç†æƒ³çº¿\n",
    "    plt.plot(mean_pred_dev, frac_pos_dev, marker=\"o\", label=\"Development (OOF)\")\n",
    "    plt.plot(mean_pred_te,  frac_pos_te,  marker=\"o\", label=\"validation\")\n",
    "\n",
    "    plt.xlabel(\"Mean predicted probability\")\n",
    "    plt.ylabel(\"Observed event rate\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(f\"{out_prefix}.png\", dpi=600, bbox_inches=\"tight\")\n",
    "    plt.savefig(f\"{out_prefix}.tiff\", dpi=600, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "# ===================== è¯»æ•°æ® =====================\n",
    "df_dev = pd.read_excel(DEV_XLSX)\n",
    "df_te  = pd.read_excel(TEST_XLSX)\n",
    "\n",
    "y_dev = pd.to_numeric(df_dev[TARGET_COL], errors=\"coerce\").astype(int).values\n",
    "y_te  = pd.to_numeric(df_te[TARGET_COL], errors=\"coerce\").astype(int).values\n",
    "\n",
    "X_dev = to_numeric_no_missing(df_dev, FEATURES, name=\"X_dev\")\n",
    "X_te  = to_numeric_no_missing(df_te,  FEATURES, name=\"X_val\")\n",
    "\n",
    "# ===================== åŠ è½½æ¨¡å‹ =====================\n",
    "base_model = joblib.load(MODEL_PKL)\n",
    "if not hasattr(base_model, \"predict_proba\"):\n",
    "    raise ValueError(\"Loaded model does not support predict_proba().\")\n",
    "\n",
    "# ===================== Developmentï¼šOOF predicted probabilities =====================\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "p_dev_oof = oof_predict_proba(base_model, X_dev, y_dev, cv=cv)\n",
    "\n",
    "# ===================== valï¼šfinal model fit on full dev =====================\n",
    "final_model = clone(base_model)\n",
    "final_model.fit(X_dev, y_dev)\n",
    "p_te = final_model.predict_proba(X_te)[:, 1]\n",
    "\n",
    "# ===================== æŒ‡æ ‡ï¼šBrier + slope/intercept =====================\n",
    "brier_dev = brier_score_loss(y_dev, p_dev_oof)\n",
    "brier_te  = brier_score_loss(y_te,  p_te)\n",
    "\n",
    "int_dev, slope_dev = calibration_slope_intercept(y_dev, p_dev_oof)\n",
    "int_te,  slope_te  = calibration_slope_intercept(y_te,  p_te)\n",
    "\n",
    "print(\"\\n=== Calibration metrics ===\")\n",
    "print(f\"Development (OOF): Brier={brier_dev:.4f}, intercept={int_dev:.3f}, slope={slope_dev:.3f}\")\n",
    "print(f\"validation  : Brier={brier_te:.4f}, intercept={int_te:.3f}, slope={slope_te:.3f}\")\n",
    "\n",
    "# ===================== ç”»å›¾å¹¶ä¿å­˜ï¼ˆ600 dpiï¼‰ =====================\n",
    "plot_calibration(\n",
    "    y_dev=y_dev, p_dev=p_dev_oof,\n",
    "    y_test=y_te, p_test=p_te,\n",
    "    n_bins=10,\n",
    "    out_prefix=\"Figure_Calibration_primary5_RF\"\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… Saved: Figure_Calibration_primary5_RF.png / .tiff (600 dpi)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912522d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.base import clone\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "# ===================== é…ç½® =====================\n",
    "MODEL_PKL = \"best_model_primary_5_random_forest.pkl\"   # æŒ‰ä½ å®é™…æ–‡ä»¶åä¿®æ”¹\n",
    "DEV_XLSX  = \"your_dataset.xlsx\"\n",
    "TEST_XLSX = \"your_dataset.xlsx\"\n",
    "TARGET_COL = \"group\"\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "FEATURES = [\n",
    "    'your_data'\n",
    "]\n",
    "\n",
    "# ===================== å·¥å…·å‡½æ•° =====================\n",
    "def to_numeric_no_missing(df, cols, name=\"X\"):\n",
    "    X = df[cols].copy()\n",
    "    for c in cols:\n",
    "        X[c] = pd.to_numeric(X[c], errors=\"coerce\")\n",
    "    if X.isna().any().any():\n",
    "        miss = X.isna().mean().to_dict()\n",
    "        raise ValueError(f\"{name} still has missing values: {miss}\")\n",
    "    return X\n",
    "\n",
    "def oof_predict_proba(base_model, X, y, cv):\n",
    "    oof = np.full(len(y), np.nan, dtype=float)\n",
    "    for tr, va in cv.split(X, y):\n",
    "        m = clone(base_model)\n",
    "        m.fit(X.iloc[tr], y[tr])\n",
    "        oof[va] = m.predict_proba(X.iloc[va])[:, 1]\n",
    "    if np.isnan(oof).any():\n",
    "        raise RuntimeError(\"OOF probabilities contain NaN.\")\n",
    "    return oof\n",
    "\n",
    "def bootstrap_auc_ci(y_true, y_score, n_boot=2000, seed=42):\n",
    "    \"\"\"\n",
    "    Bootstrap 95% CI for AUC.\n",
    "    - å¯¹æ¯æ¬¡bootstrapé‡é‡‡æ ·ï¼ˆæœ‰æ”¾å›ï¼‰\n",
    "    - è‹¥æŠ½æ ·åå…¨ä¸ºåŒä¸€ç±»ï¼Œè·³è¿‡è¯¥æ¬¡\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_score = np.asarray(y_score)\n",
    "\n",
    "    aucs = []\n",
    "    n = len(y_true)\n",
    "\n",
    "    for _ in range(n_boot):\n",
    "        idx = rng.integers(0, n, n)\n",
    "        y_b = y_true[idx]\n",
    "        s_b = y_score[idx]\n",
    "        if len(np.unique(y_b)) < 2:\n",
    "            continue\n",
    "        aucs.append(roc_auc_score(y_b, s_b))\n",
    "\n",
    "    if len(aucs) < 200:\n",
    "        raise RuntimeError(f\"Too few valid bootstrap samples: {len(aucs)}\")\n",
    "\n",
    "    aucs = np.sort(np.array(aucs))\n",
    "    lo = float(np.percentile(aucs, 2.5))\n",
    "    hi = float(np.percentile(aucs, 97.5))\n",
    "    return lo, hi\n",
    "\n",
    "# ===================== è¯»æ•°æ® =====================\n",
    "df_dev = pd.read_excel(DEV_XLSX)\n",
    "df_te  = pd.read_excel(TEST_XLSX)\n",
    "\n",
    "y_dev = pd.to_numeric(df_dev[TARGET_COL], errors=\"coerce\").astype(int).values\n",
    "y_te  = pd.to_numeric(df_te[TARGET_COL],  errors=\"coerce\").astype(int).values\n",
    "\n",
    "X_dev = to_numeric_no_missing(df_dev, FEATURES, name=\"X_dev\")\n",
    "X_te  = to_numeric_no_missing(df_te,  FEATURES, name=\"X_val\")\n",
    "\n",
    "# ===================== åŠ è½½æ¨¡å‹ =====================\n",
    "base_model = joblib.load(MODEL_PKL)\n",
    "if not hasattr(base_model, \"predict_proba\"):\n",
    "    raise ValueError(\"Loaded model does not support predict_proba().\")\n",
    "\n",
    "# ===================== Developmentï¼šOOF ROC =====================\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "p_dev_oof = oof_predict_proba(base_model, X_dev, y_dev, cv=cv)\n",
    "\n",
    "auc_dev = roc_auc_score(y_dev, p_dev_oof)\n",
    "ci_dev = bootstrap_auc_ci(y_dev, p_dev_oof, n_boot=2000, seed=RANDOM_STATE)\n",
    "fpr_dev, tpr_dev, _ = roc_curve(y_dev, p_dev_oof)\n",
    "\n",
    "# ===================== valï¼šFinal model fit on full dev =====================\n",
    "final_model = clone(base_model)\n",
    "final_model.fit(X_dev, y_dev)\n",
    "p_te = final_model.predict_proba(X_te)[:, 1]\n",
    "\n",
    "auc_te = roc_auc_score(y_te, p_te)\n",
    "ci_te = bootstrap_auc_ci(y_te, p_te, n_boot=2000, seed=RANDOM_STATE + 1)\n",
    "fpr_te, tpr_te, _ = roc_curve(y_te, p_te)\n",
    "\n",
    "# ===================== è¾“å‡ºæ•°å€¼ =====================\n",
    "print(\"\\n=== ROC / AUC ===\")\n",
    "print(f\"Development (OOF): AUC={auc_dev:.4f} (95% CI {ci_dev[0]:.4f}â€“{ci_dev[1]:.4f})\")\n",
    "print(f\"val validation  : AUC={auc_te:.4f} (95% CI {ci_te[0]:.4f}â€“{ci_te[1]:.4f})\")\n",
    "\n",
    "# ===================== ç”»å›¾å¹¶ä¿å­˜ï¼ˆ600 dpiï¼‰ =====================\n",
    "plt.figure()\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", linewidth=1)\n",
    "\n",
    "plt.plot(\n",
    "    fpr_dev, tpr_dev,\n",
    "    label=f\"Development (OOF) AUC {auc_dev:.3f} (95% CI {ci_dev[0]:.3f}â€“{ci_dev[1]:.3f})\"\n",
    ")\n",
    "plt.plot(\n",
    "    fpr_te, tpr_te,\n",
    "    label=f\"val validation AUC {auc_te:.3f} (95% CI {ci_te[0]:.3f}â€“{ci_te[1]:.3f})\"\n",
    ")\n",
    "\n",
    "plt.xlabel(\"False positive rate\")\n",
    "plt.ylabel(\"True positive rate\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"Figure_ROC_primary5_RF.png\", dpi=600, bbox_inches=\"tight\")\n",
    "plt.savefig(\"Figure_ROC_primary5_RF.tiff\", dpi=600, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "print(\"\\nâœ… Saved: Figure_ROC_primary5_RF.png / .tiff (600 dpi)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7e8d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.base import clone\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# ===================== é…ç½® =====================\n",
    "MODEL_PKL = \"best_model_primary_5_random_forest.pkl\"\n",
    "DEV_XLSX  = \"your_dataset.xlsx\"\n",
    "TEST_XLSX = \"your_dataset.xlsx\"\n",
    "TARGET_COL = \"group\"\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "FEATURES = [\n",
    "    'your_data'\n",
    "]\n",
    "\n",
    "# ===================== å·¥å…·å‡½æ•° =====================\n",
    "def to_numeric_no_missing(df, cols):\n",
    "    X = df[cols].copy()\n",
    "    for c in cols:\n",
    "        X[c] = pd.to_numeric(X[c], errors=\"coerce\")\n",
    "    if X.isna().any().any():\n",
    "        miss = X.isna().mean().to_dict()\n",
    "        raise ValueError(f\"Missing values detected: {miss}\")\n",
    "    return X\n",
    "\n",
    "def oof_predict_proba(base_model, X, y):\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "    oof = np.zeros(len(y), dtype=float)\n",
    "    for tr, va in cv.split(X, y):\n",
    "        m = clone(base_model)\n",
    "        m.fit(X.iloc[tr], y[tr])\n",
    "        oof[va] = m.predict_proba(X.iloc[va])[:, 1]\n",
    "    return oof\n",
    "\n",
    "def decision_curve(y_true, prob, thresholds):\n",
    "    y_true = np.asarray(y_true).astype(int)\n",
    "    prob = np.asarray(prob).astype(float)\n",
    "\n",
    "    n = len(y_true)\n",
    "    prev = np.mean(y_true == 1)\n",
    "\n",
    "    nb_model = []\n",
    "    for pt in thresholds:\n",
    "        # æ•°å€¼ç¨³å®šï¼šé¿å… pt=1 å¯¼è‡´çˆ†ç‚¸\n",
    "        if pt <= 0 or pt >= 1:\n",
    "            nb_model.append(np.nan)\n",
    "            continue\n",
    "\n",
    "        pred = (prob >= pt).astype(int)\n",
    "        tp = np.sum((pred == 1) & (y_true == 1))\n",
    "        fp = np.sum((pred == 1) & (y_true == 0))\n",
    "\n",
    "        w = pt / (1 - pt)\n",
    "        nb = (tp / n) - (fp / n) * w\n",
    "        nb_model.append(nb)\n",
    "\n",
    "    nb_model = np.array(nb_model, dtype=float)\n",
    "    nb_all = prev - (1 - prev) * (thresholds / (1 - thresholds))\n",
    "    nb_none = np.zeros_like(thresholds, dtype=float)\n",
    "\n",
    "    return nb_model, nb_all, nb_none\n",
    "\n",
    "# ===================== è¯»å–æ•°æ® =====================\n",
    "df_dev = pd.read_excel(DEV_XLSX)\n",
    "df_te  = pd.read_excel(TEST_XLSX)\n",
    "\n",
    "y_dev = pd.to_numeric(df_dev[TARGET_COL], errors=\"coerce\").astype(int).values\n",
    "y_te  = pd.to_numeric(df_te[TARGET_COL], errors=\"coerce\").astype(int).values\n",
    "\n",
    "X_dev = to_numeric_no_missing(df_dev, FEATURES)\n",
    "X_te  = to_numeric_no_missing(df_te, FEATURES)\n",
    "\n",
    "# ===================== åŠ è½½æ¨¡å‹ =====================\n",
    "base_model = joblib.load(MODEL_PKL)\n",
    "\n",
    "# ===================== Development OOF =====================\n",
    "p_dev = oof_predict_proba(base_model, X_dev, y_dev)\n",
    "\n",
    "# =====================  é¢„æµ‹ï¼ˆæœ€ç»ˆæ¨¡å‹ï¼šç”¨å…¨å¼€å‘é›†æ‹Ÿåˆåé¢„æµ‹ï¼‰ =====================\n",
    "final_model = clone(base_model)\n",
    "final_model.fit(X_dev, y_dev)\n",
    "p_te = final_model.predict_proba(X_te)[:, 1]\n",
    "\n",
    "# ===================== DCAï¼ˆå…³é”®ï¼šé˜ˆå€¼èŒƒå›´æ›´åˆç†ï¼‰ =====================\n",
    "# å»ºè®®åŒºé—´ï¼š0.01â€“0.60ï¼Œé¿å… pt/(1-pt) åœ¨å³ä¾§çˆ†ç‚¸æ‹–å®åæ ‡è½´\n",
    "thresholds = np.arange(0.01, 0.61, 0.01)\n",
    "\n",
    "nb_dev, nb_all_dev, nb_none = decision_curve(y_dev, p_dev, thresholds)\n",
    "nb_te,  nb_all_te,  _       = decision_curve(y_te,  p_te,  thresholds)\n",
    "\n",
    "# ===================== ç”»å›¾ï¼ˆæ›´æ‚å¿—å‹å¥½ï¼‰ =====================\n",
    "plt.figure(figsize=(7.2, 5.4))\n",
    "\n",
    "plt.plot(thresholds, nb_dev, label=\"Development (OOF)\")\n",
    "plt.plot(thresholds, nb_te,  label=\"2024 validation\")\n",
    "\n",
    "# åªç”»ä¸€ä¸ª treat-allï¼ˆå»ºè®®ç”¨ testï¼‰ï¼Œé¿å…å›¾ä¾‹æ­§ä¹‰\n",
    "plt.plot(thresholds, nb_all_te, linestyle=\"--\", label=\"Treat all\")\n",
    "plt.plot(thresholds, nb_none,   linestyle=\"--\", label=\"Treat none\")\n",
    "\n",
    "plt.xlabel(\"Threshold probability\")\n",
    "plt.ylabel(\"Net benefit\")\n",
    "plt.xlim(0.0, 0.60)\n",
    "\n",
    "# å¦‚æœä»è¢«æç«¯å€¼å‹æ‰ï¼Œå†æ‰“å¼€ä¸‹ä¸€è¡Œï¼ˆä¸€èˆ¬ä¸éœ€è¦ï¼‰\n",
    "# plt.ylim(-0.05, 0.25)\n",
    "\n",
    "plt.legend(loc=\"lower left\", frameon=True)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"Figure_DCA_primary5_RF_clean.png\", dpi=600, bbox_inches=\"tight\")\n",
    "plt.savefig(\"Figure_DCA_primary5_RF_clean.tiff\", dpi=600, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "print(\"âœ… DCA figure saved: Figure_DCA_primary5_RF_clean (600 dpi)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8364f14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# ===================== é…ç½® =====================\n",
    "MODEL_PKL = \"best_model_primary_5_random_forest.pkl\"\n",
    "DEV_XLSX = \"your_dataset.xlsx\"\n",
    "TARGET_COL = \"group\"\n",
    "\n",
    "FEATURES = [\n",
    "    'your_data'\n",
    "]\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# ===================== è¯»å–æ•°æ® =====================\n",
    "df = pd.read_excel(DEV_XLSX)\n",
    "\n",
    "X = df[FEATURES].apply(pd.to_numeric)\n",
    "y = pd.to_numeric(df[TARGET_COL]).values\n",
    "\n",
    "# ===================== åŠ è½½æ¨¡å‹ =====================\n",
    "model = joblib.load(MODEL_PKL)\n",
    "\n",
    "# ===================== è®¡ç®— Permutation Importance =====================\n",
    "result = permutation_importance(\n",
    "    model,\n",
    "    X,\n",
    "    y,\n",
    "    scoring=\"roc_auc\",\n",
    "    n_repeats=30,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "importances = result.importances_mean\n",
    "std = result.importances_std\n",
    "\n",
    "# æ’åº\n",
    "indices = np.argsort(importances)\n",
    "\n",
    "# ===================== ç”»å›¾ =====================\n",
    "plt.figure()\n",
    "plt.barh(range(len(indices)), importances[indices], xerr=std[indices])\n",
    "plt.yticks(range(len(indices)), np.array(FEATURES)[indices])\n",
    "plt.xlabel(\"Mean decrease in AUC (permutation importance)\")\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"Figure_PermutationImportance_primary5_RF.png\", dpi=600, bbox_inches=\"tight\")\n",
    "plt.savefig(\"Figure_PermutationImportance_primary5_RF.tiff\", dpi=600, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "print(\"âœ… Permutation importance figure saved (600 dpi)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39912e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ===================== é…ç½® =====================\n",
    "MODEL_PKL = \"best_model_primary_5_random_forest.pkl\"\n",
    "DEV_XLSX = \"your_dataset.xlsx\"\n",
    "TARGET_COL = \"group\"\n",
    "\n",
    "FEATURES = [\n",
    "    'your_data'\n",
    "]\n",
    "\n",
    "# ===================== è¯»å–æ•°æ® =====================\n",
    "df = pd.read_excel(DEV_XLSX)\n",
    "X = df[FEATURES].apply(pd.to_numeric)\n",
    "\n",
    "# ===================== åŠ è½½æ¨¡å‹ =====================\n",
    "model = joblib.load(MODEL_PKL)\n",
    "\n",
    "# å…¼å®¹ Pipeline\n",
    "if hasattr(model, \"named_steps\") and (\"clf\" in model.named_steps):\n",
    "    clf = model.named_steps[\"clf\"]\n",
    "else:\n",
    "    clf = model\n",
    "\n",
    "importances = clf.feature_importances_\n",
    "indices = np.argsort(importances)\n",
    "\n",
    "# ===================== ç”»å›¾ =====================\n",
    "plt.figure()\n",
    "plt.barh(range(len(indices)), importances[indices])\n",
    "plt.yticks(range(len(indices)), np.array(FEATURES)[indices])\n",
    "plt.xlabel(\"Mean decrease in impurity\")\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"Figure_MDI_primary5_RF.png\", dpi=600, bbox_inches=\"tight\")\n",
    "plt.savefig(\"Figure_MDI_primary5_RF.tiff\", dpi=600, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "print(\"âœ… MDI importance figure saved (600 dpi)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c43d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "MODEL_PKL = \"best_model_primary_5_random_forest.pkl\"\n",
    "DEV_XLSX  = \"your_dataset.xlsx\"\n",
    "\n",
    "FEATURES = [\n",
    "    'your_data'\n",
    "]\n",
    "\n",
    "OUT_PNG  = \"Figure_SHAP_summary_primary5_RF.png\"\n",
    "OUT_TIFF = \"Figure_SHAP_summary_primary5_RF.tiff\"\n",
    "\n",
    "# ---- data ----\n",
    "df = pd.read_excel(DEV_XLSX)\n",
    "X = df[FEATURES].copy()\n",
    "for c in FEATURES:\n",
    "    X[c] = pd.to_numeric(X[c], errors=\"coerce\")\n",
    "X = X.dropna().reset_index(drop=True)\n",
    "print(\"X shape:\", X.shape)\n",
    "\n",
    "# ---- model ----\n",
    "model = joblib.load(MODEL_PKL)\n",
    "clf = model.named_steps[\"clf\"] if hasattr(model, \"named_steps\") else model\n",
    "\n",
    "explainer = shap.TreeExplainer(clf)\n",
    "sv = explainer.shap_values(X)\n",
    "\n",
    "# äºŒåˆ†ç±» list -> å–æ­£ç±»\n",
    "if isinstance(sv, list):\n",
    "    sv = sv[1]\n",
    "\n",
    "arr = np.asarray(sv)\n",
    "print(\"Raw SHAP ndim/shape:\", arr.ndim, arr.shape)\n",
    "\n",
    "# ===== å…³é”®ï¼šæŠŠå„ç§è¿”å›ç»Ÿä¸€æˆ (n, p) =====\n",
    "# Case A: (n, p) -> OK\n",
    "if arr.ndim == 2:\n",
    "    shap_2d = arr\n",
    "\n",
    "# Case B: (n, p, 2) -> è¿™æ˜¯â€œè¾“å‡ºç»´åº¦=2â€ï¼Œå–æ­£ç±»é€šé“\n",
    "elif arr.ndim == 3 and arr.shape[2] == 2 and arr.shape[1] == len(FEATURES):\n",
    "    shap_2d = arr[:, :, 1]\n",
    "\n",
    "# Case C: (n, p, p) -> äº¤äº’çŸ©é˜µï¼Œèšåˆæˆæ¯ä¸ªç‰¹å¾è´¡çŒ®ï¼ˆmain+interactionï¼‰\n",
    "elif arr.ndim == 3 and arr.shape[1] == arr.shape[2] == len(FEATURES):\n",
    "    shap_2d = arr.sum(axis=2)\n",
    "\n",
    "else:\n",
    "    raise ValueError(f\"Unrecognized SHAP shape: {arr.shape}\")\n",
    "\n",
    "print(\"Converted SHAP shape:\", shap_2d.shape)\n",
    "\n",
    "# ---- beeswarm ----\n",
    "plt.figure()\n",
    "shap.summary_plot(shap_2d, X, plot_type=\"dot\", show=False)\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUT_PNG, dpi=600, bbox_inches=\"tight\")\n",
    "plt.savefig(OUT_TIFF, dpi=600, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "print(f\"âœ… Saved: {OUT_PNG} / {OUT_TIFF} (600 dpi)\")\n",
    "\n",
    "# ---- mean(|SHAP|) ----\n",
    "mean_abs = np.mean(np.abs(shap_2d), axis=0)\n",
    "rank_df = pd.DataFrame({\"Feature\": FEATURES, \"MeanAbsSHAP\": mean_abs}).sort_values(\n",
    "    \"MeanAbsSHAP\", ascending=False\n",
    ").reset_index(drop=True)\n",
    "\n",
    "print(\"\\n=== SHAP mean(|value|) ranking (class=1) ===\")\n",
    "print(rank_df)\n",
    "\n",
    "rank_df.to_csv(\"SHAP_mean_abs_ranking_primary5_RF.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "print(\"\\nâœ… Saved: SHAP_mean_abs_ranking_primary5_RF.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4e1532",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.base import clone\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# ===================== é…ç½® =====================\n",
    "MODEL_PKL = \"best_model_primary_5_random_forest.pkl\"\n",
    "DEV_XLSX = \"your_dataset.xlsx\"\n",
    "TEST_XLSX = \"your_dataset.xlsx\"\n",
    "TARGET_COL = \"group\"\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "FEATURES = [\n",
    "    'your_data'\n",
    "]\n",
    "\n",
    "# é£é™©åˆ†å±‚é˜ˆå€¼ï¼ˆæ¨èï¼‰\n",
    "LOW_CUT = 0.20\n",
    "HIGH_CUT = 0.50\n",
    "\n",
    "# ===================== å·¥å…·å‡½æ•° =====================\n",
    "def to_numeric(df, cols):\n",
    "    X = df[cols].copy()\n",
    "    for c in cols:\n",
    "        X[c] = pd.to_numeric(X[c], errors=\"coerce\")\n",
    "    return X.dropna()\n",
    "\n",
    "def oof_predict_proba(base_model, X, y):\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "    oof = np.zeros(len(y))\n",
    "    for tr, va in cv.split(X, y):\n",
    "        m = clone(base_model)\n",
    "        m.fit(X.iloc[tr], y[tr])\n",
    "        oof[va] = m.predict_proba(X.iloc[va])[:, 1]\n",
    "    return oof\n",
    "\n",
    "def assign_risk_group(p):\n",
    "    return np.where(p < LOW_CUT, \"Low\",\n",
    "           np.where(p <= HIGH_CUT, \"Intermediate\", \"High\"))\n",
    "\n",
    "def compute_event_rate(y, p):\n",
    "    groups = assign_risk_group(p)\n",
    "    df = pd.DataFrame({\"y\": y, \"group\": groups})\n",
    "    return df.groupby(\"group\")[\"y\"].mean()\n",
    "\n",
    "# ===================== è¯»å–æ•°æ® =====================\n",
    "df_dev = pd.read_excel(DEV_XLSX)\n",
    "df_te  = pd.read_excel(TEST_XLSX)\n",
    "\n",
    "y_dev = pd.to_numeric(df_dev[TARGET_COL]).values\n",
    "y_te  = pd.to_numeric(df_te[TARGET_COL]).values\n",
    "\n",
    "X_dev = to_numeric(df_dev, FEATURES)\n",
    "X_te  = to_numeric(df_te, FEATURES)\n",
    "\n",
    "# å¯¹é½ yï¼ˆé˜²æ­¢ dropna åé”™ä½ï¼‰\n",
    "y_dev = y_dev[X_dev.index]\n",
    "y_te  = y_te[X_te.index]\n",
    "\n",
    "# ===================== åŠ è½½æ¨¡å‹ =====================\n",
    "model = joblib.load(MODEL_PKL)\n",
    "\n",
    "# Development OOF\n",
    "p_dev = oof_predict_proba(model, X_dev, y_dev)\n",
    "\n",
    "# val\n",
    "final_model = clone(model)\n",
    "final_model.fit(X_dev, y_dev)\n",
    "p_te = final_model.predict_proba(X_te)[:, 1]\n",
    "\n",
    "# ===================== è®¡ç®—çœŸå®å‘ç”Ÿç‡ =====================\n",
    "rate_dev = compute_event_rate(y_dev, p_dev)\n",
    "rate_te  = compute_event_rate(y_te, p_te)\n",
    "\n",
    "print(\"\\nDevelopment event rate by risk group:\")\n",
    "print(rate_dev)\n",
    "\n",
    "print(\"\\nval event rate by risk group:\")\n",
    "print(rate_te)\n",
    "\n",
    "# ===================== ç”»å›¾ =====================\n",
    "labels = [\"Low\", \"Intermediate\", \"High\"]\n",
    "\n",
    "dev_vals = [rate_dev.get(g, 0) for g in labels]\n",
    "te_vals  = [rate_te.get(g, 0) for g in labels]\n",
    "\n",
    "x = np.arange(len(labels))\n",
    "width = 0.35\n",
    "\n",
    "plt.figure()\n",
    "plt.bar(x - width/2, dev_vals, width, label=\"Development (OOF)\")\n",
    "plt.bar(x + width/2, te_vals, width, label=\" validation\")\n",
    "\n",
    "plt.xticks(x, labels)\n",
    "plt.ylabel(\"Observed necrotizing pneumonia rate\")\n",
    "plt.xlabel(\"Predicted risk group\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"Figure_RiskStratification_primary5_RF.png\", dpi=600, bbox_inches=\"tight\")\n",
    "plt.savefig(\"Figure_RiskStratification_primary5_RF.tiff\", dpi=600, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "print(\"âœ… Risk stratification figure saved (600 dpi)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
